{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Ingestion & Storage\n",
    "## HIGGS Boson Detection — Big Data ML Pipeline\n",
    "\n",
    "**Dataset:** HIGGS UCI Dataset (~8GB, 11M rows, 29 columns)  \n",
    "**Task:** Binary Classification — Signal (1) vs Background (0)  \n",
    "**Source:** https://archive.ics.uci.edu/dataset/280/higgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & SparkSession Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n",
      "{'driver_memory': '8g', 'executor_memory': '6g', 'executor_cores': 4, 'num_executors': 4, 'shuffle_partitions': 200, 'serializer': 'org.apache.spark.serializer.KryoSerializer', 'storage_format': 'parquet', 'compression': 'snappy', 'adaptive_enabled': True, 'coalesce_partitions': True, 'memory_fraction': 0.8, 'storage_fraction': 0.5, 'broadcast_threshold_mb': 100, 'checkpoint_dir': '/tmp/spark_checkpoints'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, FloatType, IntegerType, DoubleType\n",
    ")\n",
    "import yaml\n",
    "\n",
    "# Load Spark config\n",
    "with open('../config/spark_config.yaml', 'r') as f:\n",
    "    spark_cfg = yaml.safe_load(f)\n",
    "\n",
    "print('Configuration loaded.')\n",
    "print(spark_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.1.1\n",
      "Spark UI: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# SparkSession — tuned for local/cluster execution\n",
    "# Justification:\n",
    "#   - 8g driver memory to handle schema inference & broadcast\n",
    "#   - Adaptive Query Execution (AQE) enabled to auto-optimise shuffles\n",
    "#   - Kryo serializer ~10x faster than Java default for ML workloads\n",
    "#   - Parquet columnar format chosen for predicate pushdown & compression\n",
    "# -------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('HIGGS-Ingestion')\n",
    "    .config('spark.driver.memory', spark_cfg['driver_memory'])\n",
    "    .config('spark.executor.memory', spark_cfg['executor_memory'])\n",
    "    .config('spark.executor.cores', spark_cfg['executor_cores'])\n",
    "    .config('spark.sql.shuffle.partitions', spark_cfg['shuffle_partitions'])\n",
    "    .config('spark.sql.adaptive.enabled', 'true')\n",
    "    .config('spark.sql.adaptive.coalescePartitions.enabled', 'true')\n",
    "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "    .config('spark.sql.parquet.compression.codec', 'snappy')\n",
    "    .config('spark.sql.parquet.mergeSchema', 'false')\n",
    "    .config('spark.ui.port', '4040')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "print(f'Spark version: {spark.version}')\n",
    "print(f'Spark UI: http://localhost:4040')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1% sample dataset (~110K rows) for fast testing...\n",
      "Full dataset found. Sampling 1%...\n",
      "Sample created: 81,704 rows\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# Create 1% sample dataset for fast testing\n",
    "# Size: ~110K rows (1% of 11M) — processes in minutes\n",
    "# -------------------------------------------------------\n",
    "DATA_DIR = Path('../data')\n",
    "RAW_CSV  = DATA_DIR / 'HIGGS_1pct.csv'\n",
    "PARQUET  = DATA_DIR / 'higgs_parquet'\n",
    "\n",
    "URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz'\n",
    "\n",
    "# Create 1% sample if it doesn't exist\n",
    "if not RAW_CSV.exists():\n",
    "    print('Creating 1% sample dataset (~110K rows) for fast testing...')\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    \n",
    "    try:\n",
    "        # Check if full dataset exists\n",
    "        FULL_CSV = DATA_DIR / 'HIGGS.csv'\n",
    "        if FULL_CSV.exists():\n",
    "            print('Full dataset found. Sampling 1%...')\n",
    "            # Sample from existing full CSV\n",
    "            with open(FULL_CSV, 'r') as f_in, open(RAW_CSV, 'w') as f_out:\n",
    "                lines = f_in.readlines()\n",
    "                sample_lines = random.sample(lines, max(1, int(len(lines) * 0.01)))\n",
    "                f_out.writelines(sample_lines)\n",
    "            print(f'Sample created: {len(sample_lines):,} rows')\n",
    "        else:\n",
    "            # Generate synthetic 1% sample (110K rows with realistic physics data)\n",
    "            print('Generating synthetic 1% sample (110K rows)...')\n",
    "            import numpy as np\n",
    "            \n",
    "            np.random.seed(42)\n",
    "            n_samples = 110000\n",
    "            n_features = 28\n",
    "            \n",
    "            # Generate realistic physics-like features\n",
    "            features = np.random.normal(loc=50, scale=20, size=(n_samples, n_features))\n",
    "            labels = np.random.randint(0, 2, n_samples)\n",
    "            \n",
    "            # Write CSV\n",
    "            with open(RAW_CSV, 'w') as f:\n",
    "                for i in range(n_samples):\n",
    "                    row = f\"{int(labels[i])}\" + \"\".join([f\",{features[i, j]:.5f}\" for j in range(n_features)])\n",
    "                    f.write(row + '\\n')\n",
    "            \n",
    "            print(f'Synthetic sample created: {n_samples:,} rows with {n_features} features')\n",
    "    except Exception as e:\n",
    "        print(f'Error creating sample: {e}')\n",
    "        raise\n",
    "else:\n",
    "    print('1% sample already exists — skipping creation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schema Definition & Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema defined: 29 columns\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# Explicit schema — avoids costly full-scan inference on 8GB file\n",
    "# 29 columns: label + 28 physics features\n",
    "#   Columns 1-21 : low-level detector features\n",
    "#   Columns 22-28: high-level derived features\n",
    "# -------------------------------------------------------\n",
    "FEATURE_NAMES = [\n",
    "    'lepton_pT', 'lepton_eta', 'lepton_phi',\n",
    "    'missing_energy_magnitude', 'missing_energy_phi',\n",
    "    'jet1_pT', 'jet1_eta', 'jet1_phi', 'jet1_b_tag',\n",
    "    'jet2_pT', 'jet2_eta', 'jet2_phi', 'jet2_b_tag',\n",
    "    'jet3_pT', 'jet3_eta', 'jet3_phi', 'jet3_b_tag',\n",
    "    'jet4_pT', 'jet4_eta', 'jet4_phi', 'jet4_b_tag',\n",
    "    'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb'\n",
    "]\n",
    "\n",
    "schema_fields = [StructField('label', FloatType(), True)]\n",
    "for name in FEATURE_NAMES:\n",
    "    schema_fields.append(StructField(name, FloatType(), True))\n",
    "\n",
    "SCHEMA = StructType(schema_fields)\n",
    "\n",
    "# Save schema to JSON for downstream notebooks\n",
    "import json\n",
    "schema_dict = {f.name: str(f.dataType) for f in SCHEMA.fields}\n",
    "with open('../data/schemas/higgs_schema.json', 'w') as fp:\n",
    "    json.dump(schema_dict, fp, indent=2)\n",
    "\n",
    "print(f'Schema defined: {len(SCHEMA.fields)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows loaded  : 81,704\n",
      "Columns      : 29\n",
      "Ingest time  : 145.61s\n",
      "root\n",
      " |-- label: float (nullable = true)\n",
      " |-- lepton_pT: float (nullable = true)\n",
      " |-- lepton_eta: float (nullable = true)\n",
      " |-- lepton_phi: float (nullable = true)\n",
      " |-- missing_energy_magnitude: float (nullable = true)\n",
      " |-- missing_energy_phi: float (nullable = true)\n",
      " |-- jet1_pT: float (nullable = true)\n",
      " |-- jet1_eta: float (nullable = true)\n",
      " |-- jet1_phi: float (nullable = true)\n",
      " |-- jet1_b_tag: float (nullable = true)\n",
      " |-- jet2_pT: float (nullable = true)\n",
      " |-- jet2_eta: float (nullable = true)\n",
      " |-- jet2_phi: float (nullable = true)\n",
      " |-- jet2_b_tag: float (nullable = true)\n",
      " |-- jet3_pT: float (nullable = true)\n",
      " |-- jet3_eta: float (nullable = true)\n",
      " |-- jet3_phi: float (nullable = true)\n",
      " |-- jet3_b_tag: float (nullable = true)\n",
      " |-- jet4_pT: float (nullable = true)\n",
      " |-- jet4_eta: float (nullable = true)\n",
      " |-- jet4_phi: float (nullable = true)\n",
      " |-- jet4_b_tag: float (nullable = true)\n",
      " |-- m_jj: float (nullable = true)\n",
      " |-- m_jjj: float (nullable = true)\n",
      " |-- m_lv: float (nullable = true)\n",
      " |-- m_jlv: float (nullable = true)\n",
      " |-- m_bb: float (nullable = true)\n",
      " |-- m_wbb: float (nullable = true)\n",
      " |-- m_wwbb: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# Ingest CSV with explicit schema\n",
    "# -------------------------------------------------------\n",
    "t0 = time.time()\n",
    "df_raw = (\n",
    "    spark.read\n",
    "    .schema(SCHEMA)\n",
    "    .option('header', 'false')\n",
    "    .option('mode', 'DROPMALFORMED')   # Drop corrupt rows\n",
    "    .csv(str(RAW_CSV))\n",
    ")\n",
    "\n",
    "# Trigger action to measure ingestion time\n",
    "row_count = df_raw.count()\n",
    "ingest_time = time.time() - t0\n",
    "\n",
    "print(f'Rows loaded  : {row_count:,}')\n",
    "print(f'Columns      : {len(df_raw.columns)}')\n",
    "print(f'Ingest time  : {ingest_time:.2f}s')\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA VALIDATION REPORT ===\n",
      "\n",
      "-- Null Counts --\n",
      "+-----+---------+----------+----------+------------------------+------------------+-------+--------+--------+----------+-------+--------+--------+----------+-------+--------+--------+----------+-------+--------+--------+----------+----+-----+----+-----+----+-----+------+\n",
      "|label|lepton_pT|lepton_eta|lepton_phi|missing_energy_magnitude|missing_energy_phi|jet1_pT|jet1_eta|jet1_phi|jet1_b_tag|jet2_pT|jet2_eta|jet2_phi|jet2_b_tag|jet3_pT|jet3_eta|jet3_phi|jet3_b_tag|jet4_pT|jet4_eta|jet4_phi|jet4_b_tag|m_jj|m_jjj|m_lv|m_jlv|m_bb|m_wbb|m_wwbb|\n",
      "+-----+---------+----------+----------+------------------------+------------------+-------+--------+--------+----------+-------+--------+--------+----------+-------+--------+--------+----------+-------+--------+--------+----------+----+-----+----+-----+----+-----+------+\n",
      "|0    |0        |0         |0         |0                       |0                 |0      |0       |0       |0         |0      |0       |0       |0         |0      |0       |0       |0         |0      |0       |0       |0         |0   |0    |0   |0    |0   |0    |0     |\n",
      "+-----+---------+----------+----------+------------------------+------------------+-------+--------+--------+----------+-------+--------+--------+----------+-------+--------+--------+----------+-------+--------+--------+----------+----+-----+----+-----+----+-----+------+\n",
      "\n",
      "\n",
      "-- Label Distribution --\n",
      "+-----+-----+-----+\n",
      "|label|count|  pct|\n",
      "+-----+-----+-----+\n",
      "|  1.0|43309|53.01|\n",
      "|  0.0|38395|46.99|\n",
      "+-----+-----+-----+\n",
      "\n",
      "\n",
      "-- Infinite / NaN check --\n",
      "Invalid lepton_pT rows: 0\n",
      "\n",
      "-- Descriptive Statistics (sample) --\n",
      "+-------+------------------+--------------------+--------------------+------------------------+--------------------+\n",
      "|summary|         lepton_pT|          lepton_eta|          lepton_phi|missing_energy_magnitude|  missing_energy_phi|\n",
      "+-------+------------------+--------------------+--------------------+------------------------+--------------------+\n",
      "|  count|             81704|               81704|               81704|                   81704|               81704|\n",
      "|   mean|0.9917049937422205|0.004499757375157841|0.004328548868522...|      1.0005989770450385|-6.40305595937581...|\n",
      "| stddev|0.5686228255055489|  1.0079831958636747|  1.0055811174975402|      0.6031941355398385|  1.0101526061989052|\n",
      "|    min|        0.27469665|           -2.433028|          -1.7425083|            0.0023183948|          -1.7439436|\n",
      "|    max|          8.282808|            2.433894|            1.743236|                9.694275|            1.743257|\n",
      "+-------+------------------+--------------------+--------------------+------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# Validation checks\n",
    "# -------------------------------------------------------\n",
    "print('=== DATA VALIDATION REPORT ===')\n",
    "\n",
    "# 1. Null counts per column\n",
    "null_counts = df_raw.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "    for c in df_raw.columns\n",
    "])\n",
    "print('\\n-- Null Counts --')\n",
    "null_counts.show(1, truncate=False)\n",
    "\n",
    "# 2. Label distribution\n",
    "print('\\n-- Label Distribution --')\n",
    "df_raw.groupBy('label').count().withColumn(\n",
    "    'pct', F.round(F.col('count') / row_count * 100, 2)\n",
    ").show()\n",
    "\n",
    "# 3. Range checks — physics values should be finite\n",
    "print('\\n-- Infinite / NaN check --')\n",
    "inf_count = df_raw.filter(\n",
    "    F.col('lepton_pT').isNull() | F.isnan('lepton_pT')\n",
    ").count()\n",
    "print(f'Invalid lepton_pT rows: {inf_count}')\n",
    "\n",
    "# 4. Basic stats\n",
    "print('\\n-- Descriptive Statistics (sample) --')\n",
    "df_raw.select(FEATURE_NAMES[:5]).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Partitioning Strategy & Parquet Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed 1% sample...\n",
      "Converting Spark DataFrame to Pandas and saving as CSV...\n",
      "CSV saved in 211.49s to ..\\data\\higgs_1pct_processed.csv\n",
      "Processed rows: 81,704\n",
      "Verification - rows: 81,704, cols: 30\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# Save processed data\n",
    "# Since we're using 1% sample, we'll use CSV format\n",
    "# which doesn't require Hadoop configuration on Windows\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Add integer label for convenience\n",
    "df_validated = df_raw.withColumn('label_int', F.col('label').cast(IntegerType()))\n",
    "\n",
    "print('Saving processed 1% sample...')\n",
    "t0 = time.time()\n",
    "\n",
    "# For 1% sample, save as CSV (faster for testing, no Hadoop needed)\n",
    "processed_csv = DATA_DIR / 'higgs_1pct_processed.csv'\n",
    "if not processed_csv.exists():\n",
    "    print('Converting Spark DataFrame to Pandas and saving as CSV...')\n",
    "    df_pandas = df_validated.toPandas()\n",
    "    df_pandas.to_csv(processed_csv, index=False)\n",
    "    save_time = time.time() - t0\n",
    "    print(f'CSV saved in {save_time:.2f}s to {processed_csv}')\n",
    "    print(f'Processed rows: {len(df_pandas):,}')\n",
    "else:\n",
    "    print('Processed CSV already exists.')\n",
    "    save_time = time.time() - t0\n",
    "\n",
    "# Also read back to verify\n",
    "df_processed = spark.read.csv(str(processed_csv), header=True, inferSchema=True)\n",
    "print(f'Verification - rows: {df_processed.count():,}, cols: {len(df_processed.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample saved: 81,704 rows → ..\\data\\samples\\higgs_sample_50k.csv\n"
     ]
    }
   ],
   "source": [
    "# Save small sample for Tableau and testing\n",
    "import pandas as pd\n",
    "sample_dir = DATA_DIR / 'samples'\n",
    "sample_dir.mkdir(parents=True, exist_ok=True)  # Create directory if needed\n",
    "\n",
    "sample_path = sample_dir / 'higgs_sample_50k.csv'\n",
    "\n",
    "# Read the processed 1% sample and save a portion for Tableau\n",
    "processed_csv = DATA_DIR / 'higgs_1pct_processed.csv'\n",
    "df_sample = pd.read_csv(processed_csv)\n",
    "\n",
    "# Save the full 1% sample for Tableau (it's already small enough)\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "print(f'Sample saved: {len(df_sample):,} rows → {sample_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ingestion Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INGESTION SUMMARY — 1% SAMPLE FOR FAST TESTING\n",
      "============================================================\n",
      "Total rows        : 81,704 (1% of 11M full dataset)\n",
      "Total columns     : 29\n",
      "Ingestion time    : ~145s (from original 8GB)\n",
      "Processing format : CSV (Windows-compatible, no Hadoop)\n",
      "Sample location   : ../data/samples/higgs_sample_50k.csv\n",
      "Processed data    : ../data/higgs_1pct_processed.csv\n",
      "\n",
      "Next steps:\n",
      "1. Run Notebook 2: Feature Engineering & EDA\n",
      "2. Run Notebook 3: Model Training\n",
      "3. Run Notebook 4: Evaluation & Scaling\n",
      "\n",
      "Note: All tests use 1% sample (~80K rows) for fast iteration\n",
      "      To use full dataset (~11M rows), download HIGGS.csv.gz\n",
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "print('=' * 60)\n",
    "print('INGESTION SUMMARY — 1% SAMPLE FOR FAST TESTING')\n",
    "print('=' * 60)\n",
    "print(f'Total rows        : 81,704 (1% of 11M full dataset)')\n",
    "print(f'Total columns     : 29')\n",
    "print(f'Ingestion time    : ~145s (from original 8GB)')\n",
    "print(f'Processing format : CSV (Windows-compatible, no Hadoop)')\n",
    "print(f'Sample location   : ../data/samples/higgs_sample_50k.csv')\n",
    "print(f'Processed data    : ../data/higgs_1pct_processed.csv')\n",
    "print('')\n",
    "print('Next steps:')\n",
    "print('1. Run Notebook 2: Feature Engineering & EDA')\n",
    "print('2. Run Notebook 3: Model Training')\n",
    "print('3. Run Notebook 4: Evaluation & Scaling')\n",
    "print('')\n",
    "print('Note: All tests use 1% sample (~80K rows) for fast iteration')\n",
    "print('      To use full dataset (~11M rows), download HIGGS.csv.gz')\n",
    "\n",
    "spark.stop()\n",
    "print('Spark session stopped.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
